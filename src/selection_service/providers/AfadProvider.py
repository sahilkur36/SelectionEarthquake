import io
import os
import time
from typing import Any, Dict, List, Type
import zipfile
import aiohttp
import pandas as pd
import requests
from ..providers.IProvider import IDataProvider
from ..enums.Enums import ProviderName
from ..processing.Mappers import IColumnMapper
from ..processing.Selection import SearchCriteria
from ..core.ErrorHandle import NetworkError, ProviderError
from ..processing.ResultHandle import async_result_decorator, result_decorator


class AFADDataProvider(IDataProvider):
    """AFAD veri saÄŸlayÄ±cÄ±"""

    def __init__(self, column_mapper: Type[IColumnMapper], timeout: int = 30):
        self.timeout = timeout
        self.column_mapper = column_mapper
        self.name = ProviderName.AFAD.value
        self.base_url = "https://ivmeservis.afad.gov.tr/Waveforms/GetWaveforms"
        self.base_download_dir = "Afad_events"
        self.mapped_df = None
        self.response_df = None
        self.headers = {
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json',
            'Origin': 'https://tadas.afad.gov.tr',
            'Referer': 'https://tadas.afad.gov.tr/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Username': 'GuestUser',
            'IsGuest': 'true'
        }

    def map_criteria(self, criteria: SearchCriteria) -> Dict[str, Any]:
        """Genel arama kriterlerini provider'a Ã¶zel formata dÃ¶nÃ¼ÅŸtÃ¼r"""
        return criteria.to_afad_params()

    @async_result_decorator
    async def fetch_data_async(self, criteria: Dict[str, Any]) -> pd.DataFrame:
        """AFAD verilerini getir"""
        try:
            payload = criteria
            print(f"AFAD arama kriterleri: {payload}")

            async with aiohttp.ClientSession(headers=self.headers) as session:
                async with session.post(
                    self.base_url,
                    json=payload,
                    timeout=self.timeout
                ) as response:
                    if response.status == 200:
                        data = await response.json() #AFAD API'si JSON formatÄ±nda veri dÃ¶ndÃ¼rÃ¼yor
                        self.response_df = pd.DataFrame(data) #JSON verisini DataFrame'e dÃ¶nÃ¼ÅŸtÃ¼r
                        self.mapped_df = self.column_mapper.map_columns(df=self.response_df) #Verileri standart kolonlara eÅŸleÅŸtir
                        self.mapped_df['PROVIDER'] = str(self.name) #SaÄŸlayÄ±cÄ± adÄ±nÄ± ekle
                        print(f"AFAD'dan {len(self.mapped_df)} kayÄ±t alÄ±ndÄ±.")
                        self.mapped_df['ENDPOINTSOURCE'] = "https://tadas.afad.gov.tr/waveform-detail/" + self.mapped_df['RSN'].astype(str) #KayÄ±tlara AFAD detay sayfasÄ± linki ekle
                        return self.mapped_df
                    else:
                        error_text = await response.text()
                        raise NetworkError(
                            self.name,
                            Exception(f"HTTP {response.status}: {error_text}"),
                            "AFAD API request failed"
                        )
        except aiohttp.ClientError as e:
            raise NetworkError(self.name, e, "AFAD network error")
        except Exception as e:
            raise ProviderError(self.name, e, f"AFAD data processing failed: {e}")

    @result_decorator
    def fetch_data_sync(self, criteria: Dict[str, Any]) -> pd.DataFrame:
        """AFAD verilerini getir (senkron)"""
        try:
            response = self._search_afad(criteria=criteria,
                                         headers=self.headers)
            if response.status_code == 200:
                data = response.json()
                self.response_df = pd.DataFrame(data)
                self.mapped_df = self.column_mapper.map_columns(df=self.response_df)
                self.mapped_df['PROVIDER'] = str(self.name)
                print(f"AFAD'dan {len(self.mapped_df)} kayÄ±t alÄ±ndÄ±.")
                return self.mapped_df
            else:
                raise NetworkError(
                    self.name,
                    Exception(f"HTTP {response.status_code}: {response.text}"),
                    "AFAD API request failed"
                )
        except requests.RequestException as e:
            raise NetworkError(self.name, e, "AFAD network error")
        except Exception as e:
            raise ProviderError(self.name, e, f"AFAD data processing failed: {e}")

    def _search_afad(self,
                     criteria: Dict[str, Any],
                     headers: dict) -> requests.Response:
        """AFAD API'sini kullanarak arama yap"""
        payload = criteria
        print(f"AFAD arama kriterleri: {payload}")
                
        response = requests.post(
            self.base_url,
            json=payload,
            headers=headers,
            timeout=self.timeout
        )
        
        return response

    def get_name(self) -> str:
        return str(self.name)

    @result_decorator
    def get_event_details(self, event_ids: List[int]) -> pd.DataFrame:
        """Birden fazla event iÃ§in detaylÄ± bilgileri alÄ±r"""
        all_details = []
        
        for event_id in event_ids:
            detail_url = f"https://ivmeservis.afad.gov.tr/Event/GetEventById/{event_id}"
            
            headers = {
                'Accept': 'application/json, text/plain, */*',
                'Origin': 'https://tadas.afad.gov.tr',
                'Referer': f'https://tadas.afad.gov.tr/event-detail/{event_id}',
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                'Username': 'GuestUser',
                'IsGuest': 'true'
            }
            
            try:
                response = requests.get(url=detail_url, headers=headers, timeout=30)
                if response.status_code == 200:
                    detail_data = response.json()
                    if isinstance(detail_data, dict):
                        all_details.append(detail_data)
                    elif isinstance(detail_data, list) and len(detail_data) > 0:
                        all_details.append(detail_data[0])
                
                time.sleep(0.1)
                
            except Exception as e:
                raise ProviderError(self.name, e, f"Event {event_id} details failed")
        
        return pd.DataFrame(all_details) if all_details else pd.DataFrame()

    def _waveform_folder_route(self, event_id: int) -> str:
        """AFAD dalga formu dosyalarÄ±nÄ±n kaydedileceÄŸi klasÃ¶r yapÄ±sÄ±nÄ± oluÅŸturur"""
        event_dir = os.path.join(self.base_download_dir, f"event_{event_id}")
        os.makedirs(event_dir, exist_ok=True)
        return event_dir
    
    def save_waveform_zipfile(self, zip_content: bytes, event_id: int, station_id: str) -> str:
        """AFAD'dan indirilen zip dosyasÄ±nÄ± kaydeder geriye dosya yolunu dÃ¶ndÃ¼rÃ¼r
        Args:
            zip_content (bytes): AFAD API'sinden gelen zip dosyasÄ±nÄ±n iÃ§eriÄŸi
            event_id (int): Ä°lgili deprem olayÄ±nÄ±n ID'si (klasÃ¶r yapÄ±sÄ± iÃ§in)
            station_id (str): Ä°stasyon kodu (dosya adlandÄ±rmasÄ± iÃ§in)
        """
        folder_dir = self._waveform_folder_route(event_id=event_id)
        zip_path = os.path.join(folder_dir, f"waveforms_{event_id}_{station_id}.zip")
        with open(zip_path, 'wb') as f:
            f.write(zip_content)
        return zip_path

    def extract_and_organize_zip(self, zip_path: str, export_type: str) -> List[str]:
        """Zip dosyasÄ±nÄ± aÃ§ar ve iÃ§indeki dosyalarÄ± organize eder"""
        extracted_files = []
        MIN_ZIP_SIZE = 1024  # bytes, treat much smaller files as suspicious

        # Quick size sanity check
        try:
            size = os.path.getsize(zip_path)
            if size < MIN_ZIP_SIZE:
                raise ProviderError(self.name, None, f"[WARNING] Ä°ndirilen zip dosyasÄ± Ã§ok kÃ¼Ã§Ã¼k ({size} bytes): {zip_path}")
        except OSError:
            # If we can't stat the file, let the normal zip handling surface the error
            pass

        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                # Test for first bad file inside the zip (returns name of bad file or None)
                bad_file = zip_ref.testzip()
                if bad_file is not None:
                    raise ProviderError(self.name, None, f"[ERROR] HasarlÄ± zip iÃ§eriÄŸi: {bad_file} in {zip_path}")

                # Extract all top-level files into the same folder as the zip
                target_dir = os.path.dirname(zip_path)
                zip_ref.extractall(target_dir)

                for file_info in zip_ref.infolist():
                    member_name = file_info.filename
                    abs_path = os.path.join(target_dir, member_name)

                    # If the member is itself a zip and ascii export requested, extract nested zip
                    if member_name.endswith('.zip') and export_type in ("asc", "asc2"):
                        try:
                            # Read inner zip bytes and open with BytesIO for safety
                            inner_bytes = zip_ref.read(member_name)
                            with zipfile.ZipFile(io.BytesIO(inner_bytes), 'r') as inner_zip:
                                inner_zip.extractall(target_dir)
                                extracted_files.extend([os.path.join(target_dir, f) for f in inner_zip.namelist()])
                        except zipfile.BadZipFile:
                            raise ProviderError(self.name, None, f"[ERROR] Ä°Ã§ zip hasarlÄ±: {member_name} inside {zip_path}")
                        except Exception as e:
                            # don't stop processing other files for a single nested failure
                            print(f"[ERROR] Ä°Ã§ zip Ã§Ä±karma hatasÄ±: {member_name} -> {e}")
                            continue
                    else:
                        extracted_files.append(abs_path)

        except zipfile.BadZipFile:
            raise ProviderError(self.name, None, f"[ERROR] HasarlÄ± zip dosyasÄ±: {zip_path}")
        except ProviderError:
            # re-raise ProviderError unchanged
            raise
        except Exception as e:
            raise ProviderError(self.name, e, f"Zip extraction failed: {e}")

        # Optionally remove the original zip if extraction succeeded
        try:
            if os.path.exists(zip_path):
                os.remove(zip_path)
        except Exception:
            pass

        return extracted_files

    @result_decorator
    def download_single_waveforms(self, filename: str, **kwargs) -> bool|ProviderError:
        """
        Downloads AFAD waveform files in batches, saves them as zip files, and extracts the contents.
        Args:
            filename (str): The filename to download. "self.mapped_df iÃ§indeki FILE_NAME_H1, FILE_NAME_H2, FILE_NAME_V kolonlarÄ±nda bulunan dosya adlarÄ± Ã¼zerinden indirme yapÄ±lacak."
            file_type (str, optional): Type of file to download. Defaults to 'unprocessed'.
            file_status (str, optional): Status of the file. Defaults to 'RawAcc'. Options --> "RawAcc", "Acc", "Vel", "Disp", "ResSpecAcc", "ResSpecVel", "ResSpecDisp", "FFT", "Husid"
            export_type (str, optional): Export format for the files. Defaults to 'asc2'. Options --> asc2, mseed, asd
            user_name (str, optional): Name of the user requesting the download. Defaults to 'GuestUser'.
            event_id (str or int, optional): Event ID for organizing downloaded files. If not provided, a timestamp is used.
        Returns:
            Dict: A dictionary containing the result of the download operation, including success status and message.
        Raises:
            ProviderError: If any error occurs during the download or extraction process.
        """
        file_type   = kwargs.get('file_type', 'unprocessed')
        file_status = kwargs.get('file_status', 'RawAcc')
        export_type = kwargs.get('export_type', 'asc2')
        user_name   = kwargs.get('user_name', 'GuestUser')
        event_id    = kwargs.get('event_id', int(time.time())) # Event ID yoksa timestamp kullan
        station_id  = kwargs.get('station_code',filename.split('_')[-1] if '_' in filename else "unknown_station")

        url = "https://ivmeprocessguest.afad.gov.tr/ExportData"

        headers = {
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json',
            'Origin': 'https://tadas.afad.gov.tr',
            'Referer': 'https://tadas.afad.gov.tr/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Username': 'GuestUser',
            'IsGuest': 'true'
        }
        payload = {
                "filename": [filename],
                "file_type": [file_type],
                "file_status": file_status,
                "export_type": export_type,
                "user_name": user_name,
                "call": "afad"
            }

        try:
            # POST isteÄŸi gÃ¶nder
            response = requests.post(url, headers=headers, json=payload, timeout=50)
            response.raise_for_status()
            zip_path = self.save_waveform_zipfile(zip_content=response.content, event_id=event_id, station_id=station_id)
            extr_files = self.extract_and_organize_zip(zip_path=zip_path, export_type=export_type)
            return True
        
                
        except requests.RequestException as e:
            raise ProviderError(provider_name=self.name, original_error=e, message=f"Waveform download failed: {e}")
        
    
    @result_decorator
    def download_afad_waveforms_batch(self,
                                      filenames: List[str], **kwargs) -> Dict:
        """
        Downloads AFAD waveform files in batches, saves them as zip files, and extracts the contents.
        Args:
            filenames (List[str]): List of filenames to download.
            file_type (str, optional): Type of file to download. Defaults to 'ap'.
            file_status (str, optional): Status of the file. Defaults to 'Acc'. Options --> "RawAcc", "Acc", "Vel", "Disp", "ResSpecAcc", "ResSpecVel", "ResSpecDisp", "FFT", "Husid"
            export_type (str, optional): Export format for the files. Defaults to 'mseed'. Options --> asc2, mseed, asd
            user_name (str, optional): Name of the user requesting the download. Defaults to 'GuestUser'.
            event_id (str or int, optional): Event ID for organizing downloaded files. If not provided, a timestamp is used.
            batch_size (int, optional): Number of files per batch. Defaults to 10, maximum allowed is 10.
        Returns:
            Dict: A dictionary containing download statistics and batch results, including:
                - total_files: Total number of files requested.
                - batches: List of batch result dictionaries.
                - successful_batches: Number of batches downloaded successfully.
                - failed_batches: Number of batches that failed to download.
                - downloaded_files: Total number of files downloaded and extracted.
        Raises:
            ProviderError: If any error occurs during the download or extraction process.
        """

        file_type   = kwargs.get('file_type', 'ap')
        file_status = kwargs.get('file_status', 'Acc')
        export_type = kwargs.get('export_type', 'mseed')
        user_name   = kwargs.get('user_name', 'GuestUser')
        event_id    = kwargs.get('event_id')
        batch_size  = kwargs.get('batch_size', 10)

        batch_size = min(batch_size, 10) # Batch size'Ä± maximum 10 ile sÄ±nÄ±rla

        url = "https://ivmeprocessguest.afad.gov.tr/ExportData"

        headers = {
            'Accept': 'application/json, text/plain, */*',
            'Content-Type': 'application/json',
            'Origin': 'https://tadas.afad.gov.tr',
            'Referer': 'https://tadas.afad.gov.tr/',
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Username': 'GuestUser',
            'IsGuest': 'true'
        }
        
        all_results = {
            'total_files': len(filenames),
            'batches': [],
            'successful_batches': 0,
            'failed_batches': 0,
            'downloaded_files': 0
        }
        
        # DosyalarÄ± batch'lere ayÄ±r
        batches = [filenames[i:i + batch_size] for i in range(0, len(filenames), batch_size)]
        
        print(f"[INFO] {len(filenames)} dosya, {len(batches)} parti halinde indirilecek (max {batch_size}/parti)")
        for batch_index, batch_filenames in enumerate(batches, 1):
            print(f"[INFO] PARTÄ° {batch_index}/{len(batches)} - {len(batch_filenames)} dosya")
            
            # Request payload
            payload = {
                "filename": batch_filenames,
                "file_type": [file_type] * len(batch_filenames),
                "file_status": file_status,
                "export_type": export_type,
                "user_name": user_name,
                "call": "afad"
            }
            try:
                # POST isteÄŸi gÃ¶nder
                    response = requests.post(url, headers=headers, json=payload, timeout=50)
                    response.raise_for_status()
                    
                    # Event ID'yi kullanarak klasÃ¶r yapÄ±sÄ± oluÅŸtur
                    if event_id:
                        event_dir = os.path.join(self.base_download_dir, str(event_id))
                    else:
                        # Event ID yoksa timestamp kullan
                        event_dir = os.path.join(self.base_download_dir, f"event_{int(time.time())}")
                    
                    # # Batch klasÃ¶rÃ¼ oluÅŸtur
                    # batch_dir = os.path.join(event_dir, f"batch_{batch_index}")
                    os.makedirs(event_dir, exist_ok=True)
                    
                    # Zip dosyasÄ±nÄ± kaydet
                    zip_filename = f"part_{batch_index}.zip"
                    zip_path = os.path.join(event_dir, zip_filename)
                    
                    with open(zip_path, 'wb') as f:
                        f.write(response.content) # Zip dosyasÄ±nÄ± kaydet
                    
                    # Zip dosyasÄ±nÄ± aÃ§ ve organize et
                    extracted_files = self.extract_and_organize_zip_batch(event_path=event_dir, zip_path=zip_path, expected_filenames=batch_filenames,export_type=export_type)
                    
                    batch_result = {
                        'batch_number': batch_index,
                        'filenames': batch_filenames,
                        'batch_size': len(batch_filenames),
                        'zip_file': zip_path,
                        'extracted_files': extracted_files,
                        'extracted_count': len(extracted_files),
                        'success': True,
                        'error': None
                    }

                    # BaÅŸarÄ±sÄ±z dosyalarÄ± kontrol et ve yeniden dene
                    if len(extracted_files) < len(batch_filenames):
                        failed_files = [f for f in batch_filenames if f not in [os.path.basename(x) for x in extracted_files]]
                        if failed_files:
                            print(f"[ERROR]  {len(failed_files)} dosya Ã§Ä±karÄ±lamadÄ±, yeniden deneniyor...")
                            successful_retries = self.retry_failed_downloads(
                                event_id=event_id,
                                failed_filenames=failed_files,
                                export_type='mseed',
                                file_status=file_status
                            )
                            extracted_files.extend(successful_retries)
                    
                    all_results['batches'].append(batch_result)
                    all_results['successful_batches'] += 1
                    all_results['downloaded_files'] += len(extracted_files)
                    
                    print(f"[OK] Parti {batch_index} baÅŸarÄ±lÄ±: {len(extracted_files)} dosya")
                    
                    # Partiler arasÄ±nda bekle (sunucu yÃ¼kÃ¼nÃ¼ azaltmak iÃ§in)
                    if batch_index < len(batches):
                        wait_time = 10
                        print(f"[INFO]{wait_time} saniye bekleniyor...")
                        time.sleep(wait_time)
                    
            except Exception as e:
                raise ProviderError(self.name, e, f"Waveform download failed: {e}")

            return all_results

    def extract_and_organize_zip_batch(self,
                                   event_path: str,
                                   zip_path: str,
                                   expected_filenames: List[str],
                                   export_type: str) -> List[str]:
        """
        Zip dosyasÄ±nÄ± aÃ§ ve dosyalarÄ± organize et (batch versiyonu)
        - HasarlÄ± dosyalarÄ± tespit et ve yeniden dene
        - ASCII formatÄ±nda iÃ§ iÃ§e zip'leri Ã§Ä±kar
        - MSEED formatÄ±nÄ± dÃ¼zgÃ¼n iÅŸle
        """
        extracted_files = []
        
        try:
            # Ã–nce zip dosyasÄ±nÄ±n geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol et
            try:
                with zipfile.ZipFile(zip_path, 'r') as test_zip:
                    test_zip.testzip()  # HasarlÄ± dosyalarÄ± kontrol et
            except zipfile.BadZipFile:
                print(f"[ERROR] HasarlÄ± zip dosyasÄ±: {zip_path}")
                # HasarlÄ± dosyayÄ± sil ve None dÃ¶ndÃ¼r (yeniden deneme iÃ§in)
                try:
                    os.remove(zip_path)
                except:
                    pass
                return []

            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                # Zip iÃ§indeki tÃ¼m dosyalarÄ± listele
                zip_files = zip_ref.namelist()
                
                for filename in zip_files:
                    try:
                        # Dosya adÄ±ndan station ID'yi Ã§Ä±kar
                        if '_' in filename:
                            base_name = os.path.splitext(filename)[0]
                            parts = base_name.split('_')
                            
                            if len(parts) >= 2:
                                # Station ID'yi al (genellikle son parÃ§a)
                                station_id = parts[-1]
                                
                                # Hedef klasÃ¶rÃ¼ oluÅŸtur
                                target_dir = os.path.join(event_path, f"{station_id}")
                                os.makedirs(target_dir, exist_ok=True)
                                
                                target_path = os.path.join(target_dir, filename)
                                
                                # DosyayÄ± Ã§Ä±kar
                                with open(target_path, 'wb') as f:
                                    f.write(zip_ref.read(filename))
                                
                                # EÄŸer Ã§Ä±karÄ±lan dosya bir zip ise, iÃ§indekileri de Ã§Ä±kar
                                if filename.endswith('.zip') and export_type in ["asc","asc2"]:
                                    nested_zip_path = target_path
                                    nested_extracted = self.extract_nested_zip(nested_zip_path, target_dir)
                                    extracted_files.extend(nested_extracted)
                                    
                                    # Ä°Ã§ zip dosyasÄ±nÄ± temizle (opsiyonel)
                                    # try:
                                    #     os.remove(nested_zip_path)
                                    # except:
                                    #     pass
                                else:
                                    extracted_files.append(target_path)
                                    
                    except Exception as e:
                        print(f"[ERROR] {filename} iÅŸlenirken hata: {e}")
                        continue
            
            # BaÅŸarÄ±lÄ± Ã§Ä±karma sonrasÄ± zip'i temizle
            try:
                os.remove(zip_path)
            except:
                pass
                
        except zipfile.BadZipFile:
            print(f"[ERROR] HasarlÄ± zip dosyasÄ±: {zip_path}")
            try:
                os.remove(zip_path)
            except:
                pass
            return []
        except Exception as e:
            print(f"[ERROR] Zip aÃ§ma hatasÄ±: {e}")
        
        return extracted_files

    def retry_failed_downloads(self, event_id: int,
                               failed_filenames: List[str],
                               export_type: str,
                               file_status: str,
                               max_retries: int = 3) -> List[str]:
        """
        BaÅŸarÄ±sÄ±z indirmeleri yeniden dene
        """
        successful_downloads = []
        
        for retry in range(max_retries):
            if not failed_filenames:
                break
                
            print(f"ðŸ”„ {len(failed_filenames)} dosya iÃ§in {retry + 1}. yeniden deneme...")
            
            # 10'arli gruplar halinde yeniden dene
            batches = [failed_filenames[i:i + 10] for i in range(0, len(failed_filenames), 10)]
            
            for batch in batches:
                try:
                    result = self.download_afad_waveforms_batch(
                        event_id=event_id,
                        filenames=batch,
                        export_type=export_type,
                        file_status=file_status
                    )
                    
                    # BaÅŸarÄ±lÄ± indirmeleri listeden Ã§Ä±kar
                    if result and 'batches' in result:
                        for batch_result in result['batches']:
                            if batch_result.get('success', False):
                                successful_downloads.extend(batch_result.get('filenames', []))
                                # BaÅŸarÄ±lÄ± dosyalarÄ± failed listesinden Ã§Ä±kar
                                failed_filenames = [f for f in failed_filenames if f not in batch_result.get('filenames', [])]
                    
                    # Yeniden denemeler arasÄ±nda bekle
                    time.sleep(1)
                    
                except Exception as e:
                    print(f"[ERROR] Yeniden deneme hatasÄ±: {e}")
            
            if not failed_filenames:
                break
                
            # Sonraki deneme Ã¶ncesi bekle
            time.sleep(2)
        
        return successful_downloads

    def extract_nested_zip(self, zip_path: str, target_dir: str) -> List[str]:
        """
        Ä°Ã§ iÃ§e zip dosyalarÄ±nÄ± Ã§Ä±kar
        """
        extracted_files = []

        try:
            with zipfile.ZipFile(zip_path, 'r') as nested_zip:
                nested_files = nested_zip.namelist()

                for nested_file in nested_files:
                    try:
                        nested_target_path = os.path.join(target_dir,
                                                          nested_file)

                        # Ä°Ã§ zip'teki dosyayÄ± Ã§Ä±kar
                        with open(nested_target_path, 'wb') as f:
                            f.write(nested_zip.read(nested_file))

                        extracted_files.append(nested_target_path)

                    except Exception as e:
                        print(f"[ERROR] Ä°Ã§ zip dosyasÄ± {nested_file} iÅŸlenirken hata: {e}")
                        continue

        except Exception as e:
            print(f"[ERROR] Ä°Ã§ zip aÃ§ma hatasÄ±: {e}")

        return extracted_files
